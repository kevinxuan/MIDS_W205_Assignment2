{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## W205 Project 2: Pyspark Cluster\n",
    "#### Author: Kevin Xuan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we have entered the pyspark container. Inside this container, we will flatten the messy data ainto a structured table and remove unncessary data so that we or other data scientists can conduct further data analysis on the data and do not have to worry about dealing with unstructured or unnucessary data. First we will read data which is stored in `student-asssessment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Understand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data that is stored in the topic student-assessment\n",
    "data = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"student-assessment\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the command, we again are retrieving data from other containers, so we need to connect to kafka again with the `29092` port number. Since data is stored in kafka topic `student-assessment`, we have to specify that in our parameter. Next, we want to sort data based on time, so we specify startingoffsets and endingoffsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache this to Remove Warning messages which will appear later\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema for this dataset\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+---------+------+--------------------+-------------+\n",
      "| key|               value|             topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|student-assessment|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|student-assessment|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|student-assessment|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|student-assessment|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|student-assessment|        0|     4|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+------------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows for the data\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the value column, so we select the \"value\" column. We see that \"value\" column is encoded, so we need to convert the column to string format so that the texts are readable. To do so, we perform the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert \"value\" column to string format\n",
    "exam_data = data.select(data.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the data\n",
    "exam_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting the \"value\" column to string format, we see element in each row is a dictionary containing many keys and values. To make the data look better and into a structured table with many columns, for each row we load the dictionary and convert to a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert json column to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json python package\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that each row is a dictionary, so for each row we load the dictionary using json package\n",
    "# and convert to dataframe.\n",
    "exam = exam_data.rdd.map(lambda x: json.loads(x.value)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display first 5 rows of the data\n",
    "exam.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the new dataframe\n",
    "exam.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the schema of the exam dataset, we see that only the \"sequences\" column still consists keys and values while all others consist only values. if we remove the \"sequences\" column, then our data becomes a structured table. Hence, we will evaluate the \"sequences\" column and see if it provides extremely important information. If not, we can just remove it, which saves us a lot of work. We will first look at the \"sequences\" value of some of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sequences={'questions': [{'options': None, 'user_correct': False, 'user_incomplete': True, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': False, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}], 'id': None, 'attempt': None, 'counts': None})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: sequence of the first assessment\n",
    "exam.select('sequences').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sequences={'questions': [{'options': None, 'user_correct': False, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': False, 'user_incomplete': True, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': False, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': False, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': False, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}], 'id': None, 'attempt': None, 'counts': None})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: sequence of the 88th assessment\n",
    "exam.select('sequences').collect()[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sequences={'questions': [{'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}], 'id': None, 'attempt': None, 'counts': None})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3: sequence of the 100th assessment \n",
    "exam.select('sequences').collect()[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 3 examples above, we can see that many of the data under \"sequences\" columns are missing. For instance, although we know student answers a multiple choice question correctly (that is `user_correct = True`), we do not know which exact multiple choice question the student answers correctly (that is `id = None`). With this information, we can not derive which question is easy or which question is challenging. Moreover, the information under \"sequences\" column does not answer our business questions which are asked later, and therefore we can drop this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since \"sequence\" column does not help answer our business questions, we can remove the column from the table\n",
    "exam = exam.drop('sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have removed the column, and we look at the schema of the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exam.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the schema of the data, we see that all columns are value based, which indicates that we now have obtained a structured table in which each rectangular box contains only values and not a list or dictionary. Now we view the first 5 rows of the data to get a sense of what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows of the data\n",
    "exam.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we have a structured table, we can land table to HDFS and name the table `exam`. To do so, we run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Land table to HDFS\n",
    "exam.write.parquet(\"/tmp/exam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully save the `exam` table to cloudera HDFS with the name of `exam`. Now we register a temporary view with the table so that we can write spark SQL queries to find business insights about the data and answer our business questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register a temporary view for the table \"extracted_exam\"\n",
    "exam.registerTempTable('exam_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: How many assessments are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|num_of_assessments|\n",
      "+------------------+\n",
      "|              3242|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select count(distinct(user_exam_id)) as num_of_assessments\n",
    "    from exam_data'''\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: There are 3,242 assessments in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: What's the name of your Kafka topic? How did you come up with that name?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: The name of my Kafka topic is `student_assessment`. I come up with that name because the data is revolved around exam/assessment. Since I see that all these assessments are machine learning related, people who are taking the tests are most likely students. Hence I name the topic `student_assessment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: How many people took Learning Git?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|num_of_people_took_Learning_Git|\n",
      "+-------------------------------+\n",
      "|                            390|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select count(distinct(user_exam_id)) as num_of_people_took_Learning_Git\n",
    "    from exam_data\n",
    "    where exam_name = 'Learning Git'\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: 390 people took Learning Git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question 4: What is the least common course taken? And the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(exam_name='Learning to Visualize Data with D3.js', num_of_students=1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# least common course taken\n",
    "spark.sql('''\n",
    "    select exam_name, count(distinct(user_exam_id)) as num_of_students\n",
    "    from exam_data\n",
    "    group by exam_name\n",
    "    order by count(exam_name)\n",
    "    limit 1\n",
    "    ''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(exam_name='Learning Git', num_of_students=390)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common course\n",
    "# use collect to show entire name of the field\n",
    "spark.sql('''\n",
    "    select exam_name, count(distinct(user_exam_id)) as num_of_students\n",
    "    from exam_data\n",
    "    group by exam_name\n",
    "    order by count(exam_name) DESC\n",
    "     limit 1\n",
    "    ''').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The least common course taken is \"Learning to Visualize Data with D3.js\". The most common course taken is \"Learning Git\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  5. How many courses are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|num_of_courses|\n",
      "+--------------+\n",
      "|           103|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select count(distinct exam_name) as num_of_courses\n",
    "    from exam_data\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: There are 103 different courses in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What's the earliest and latest date for an assessment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|year|month|day|\n",
      "+----+-----+---+\n",
      "|2017|   11| 21|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select year(started_at) as year, month(started_at) as month, day(started_at) as day\n",
    "    from exam_data\n",
    "    where started_at = (select min(started_at) as min from exam_data)\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|year|month|day|\n",
      "+----+-----+---+\n",
      "|2018|    1| 28|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select year(started_at) as year, month(started_at) as month, day(started_at) as day\n",
    "    from exam_data\n",
    "    where started_at = (select max(started_at) as max from exam_data)\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: The earliest exam is on Novemember 21st in 2017, and the latest exam is January 28th on 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. How many assessments have failed certification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------+\n",
      "|certification|count(DISTINCT user_exam_id)|\n",
      "+-------------+----------------------------+\n",
      "|         null|                         132|\n",
      "|        false|                        3110|\n",
      "+-------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select certification, count(distinct(user_exam_id)) as num_of_students\n",
    "    from exam_data\n",
    "    group by certification\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: we see that 3,110 of the 3,242 assessments have failed certification, and the other 132 assessments have empty values. If the \"certification\" column represents that whether a student pass or fail the assessment, then this indicates that the dataset only records assessments in which students fail. This is an extremely important information, especially if our target variable is whether a student will pass the exam or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
